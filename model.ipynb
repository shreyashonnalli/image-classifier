{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D is spatial convolution over images\n",
    "# MaxPooling2D is a max pooling operation for 2D spatial data\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "# sequential is good for when we have a single dataset and looking for single output\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import imghdr\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import img_to_array, load_img\n",
    "from keras.models import Model\n",
    "\n",
    "from skimage import data, color, img_as_ubyte\n",
    "from skimage.feature import canny\n",
    "from skimage.transform import hough_ellipse\n",
    "from skimage.draw import ellipse_perimeter\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and gpu configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "     \n",
    "\n",
    "# removing dodgy images\n",
    "# open computer vision\n",
    "# allows us to check extensions of images\n",
    "\n",
    "# variable to data directory\n",
    "data_dir = 'tonsildb'\n",
    "\n",
    "# extensions of images\n",
    "img_exts = ['jpeg', 'jpg', 'bmp', 'png']\n",
    "\n",
    "data = tf.keras.utils.image_dataset_from_directory('tonsildb', seed=42)\n",
    "images_np = np.concatenate([x for x, y in data], axis=0)\n",
    "labels_np = np.concatenate([y for x, y in data], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available pre processing pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_correct(image):\n",
    "    image = tf.image.adjust_contrast(image, contrast_factor=1.5)\n",
    "    image = tf.image.adjust_brightness(image, delta=0.2)\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    # image = tf.cast(image, tf.float32)/255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def rgb_to_ycbcr(rgb_image):\n",
    "    R, G, B = tf.split(rgb_image, num_or_size_splits=3, axis=-1)\n",
    "    Y = 0.299 * R + 0.587 * G + 0.114 * B\n",
    "    Cb = -0.169 * R - 0.331 * G + 0.5 * B + 128\n",
    "    Cr = 0.5 * R - 0.419 * G - 0.081 * B + 128\n",
    "    ycbcr_image = tf.stack([Y, Cb, Cr], axis=3)\n",
    "    ycbcr_image = tf.squeeze(ycbcr_image, axis=-1)\n",
    "    return ycbcr_image\n",
    "\n",
    "\n",
    "def color_mask_ycbcr(ycbcr_image):\n",
    "    y, cb, cr = tf.split(ycbcr_image, num_or_size_splits=3, axis=-1)\n",
    "    y_min = 92\n",
    "    y_max = 145\n",
    "    cb_min = 112\n",
    "    cb_max = 142\n",
    "    cr_min = 135\n",
    "    cr_max = 185\n",
    "    y_mask = tf.math.logical_and(y >= y_min, y <= y_max)\n",
    "    cb_mask = tf.math.logical_and(cb >= cb_min, cb <= cb_max)\n",
    "    cr_mask = tf.math.logical_and(cr >= cr_min, cr <= cr_max)\n",
    "    color_mask = tf.math.logical_and(\n",
    "        tf.math.logical_and(y_mask, cb_mask), cr_mask)\n",
    "    color_mask = tf.cast(color_mask, dtype=tf.float32)\n",
    "    masked_image = ycbcr_image * color_mask\n",
    "    # masked_image = tf.cast(masked_image, tf.float32)/255.0\n",
    "    return masked_image\n",
    "\n",
    "def color_mask_rgb(rgb_image):\n",
    "    #print_rgb_channel_values(rgb_image)\n",
    "    r, g, b = tf.split(rgb_image, num_or_size_splits=3, axis=-1)\n",
    "    r_min = 0\n",
    "    r_max = 320\n",
    "    g_min = 0\n",
    "    g_max = 320\n",
    "    b_min = 0\n",
    "    b_max = 320\n",
    "    r_mask = tf.math.logical_and(r >= r_min, r <= r_max)\n",
    "    g_mask = tf.math.logical_and(g >= g_min, g <= g_max)\n",
    "    b_mask = tf.math.logical_and(b >= b_min, b <= b_max)\n",
    "    color_mask = tf.math.logical_and(\n",
    "        tf.math.logical_and(r_mask, g_mask), b_mask)\n",
    "    color_mask = tf.cast(color_mask, dtype=tf.float32)\n",
    "    masked_image = rgb_image * color_mask\n",
    "    # masked_image = tf.cast(masked_image, tf.float32)/255.0\n",
    "    return masked_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_for_individial_image(image):\n",
    "    image = color_correct(image=image)\n",
    "    image = rgb_to_ycbcr(rgb_image=image)\n",
    "    return image\n",
    "\n",
    "def pipeline(data):\n",
    "    data = data.map(lambda x, y: (color_correct(x), y))\n",
    "    #data = data.map(lambda x, y: (color_mask_rgb(x), y))\n",
    "    #data = data.map(lambda x, y: (rgb_to_ycbcr(x), y))\n",
    "    #data = data.map(lambda x, y: (color_mask_ycbcr(x), y))\n",
    "    return data\n",
    "\n",
    "data = pipeline(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what color correct does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_corrected_data = data.map(lambda x, y: (color_correct(x), x, y))\n",
    "for color_corrected_batch, original_batch, label_batch in color_corrected_data.take(1):\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 16, i*2 + 1)\n",
    "        plt.imshow(original_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, 16, i*2 + 2)\n",
    "        plt.imshow(color_corrected_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what rgb color mask does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_masked = data.map(lambda x, y: (color_mask_rgb(x), x, y))\n",
    "for color_corrected_batch, original_batch, label_batch in data_masked.take(1):\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 16, i*2 + 1)\n",
    "        plt.imshow(original_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, 16, i*2 + 2)\n",
    "        plt.imshow(color_corrected_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what rgb to ycbcr does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ycbcr = data.map(lambda x, y: (rgb_to_ycbcr(x), x, y))\n",
    "for color_corrected_batch, original_batch, label_batch in data_ycbcr.take(1):\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 16, i*2 + 1)\n",
    "        plt.imshow(original_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, 16, i*2 + 2)\n",
    "        plt.imshow(color_corrected_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what ycbcr color mask does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_masked = data.map(lambda x, y: (color_mask_ycbcr(x), x, y))\n",
    "for color_corrected_batch, original_batch, label_batch in data_masked.take(1):\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 16, i*2 + 1)\n",
    "        plt.imshow(original_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, 16, i*2 + 2)\n",
    "        plt.imshow(color_corrected_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what green color mask does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_masked = data.map(lambda x, y: (apply_sobel_filter(x), x, y))\n",
    "for color_corrected_batch, original_batch, label_batch in data_masked.take(1):\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 16, i*2 + 1)\n",
    "        plt.imshow(original_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, 16, i*2 + 2)\n",
    "        plt.imshow(color_corrected_batch[i]/255.0)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_image(image_np):\n",
    "    plt.imshow(image_np/255.0)\n",
    "    plt.show()\n",
    "\n",
    "def print_image_grayscale(image_np, gray = False):\n",
    "    if gray == True:\n",
    "        plt.imshow(image_np/255.0, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image_np)\n",
    "    plt.show()\n",
    "\n",
    "def combine_mask_with_image(mask, image_np):\n",
    "    image_np = cv2.bitwise_and(image_np, image_np, mask = mask)\n",
    "    return image_np\n",
    "\n",
    "\n",
    "def apply_threshold_rgb(images_np, lower = np.array([0,0,0]), upper = np.array([255,255,255])):\n",
    "    binary_np = cv2.inRange(images_np, lower, upper)\n",
    "    return binary_np\n",
    "\n",
    "\n",
    "def apply_threshold_grayscale(grayscale_np, lower=0, upper=255):\n",
    "    binary_np = cv2.inRange(grayscale_np, lower, upper)\n",
    "    return binary_np\n",
    "\n",
    "def convert_to_grayscale(images_np):\n",
    "    #gray_np = np.dot(images_np[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    gray_np = cv2.cvtColor(images_np, cv2.COLOR_BGR2GRAY)\n",
    "    return gray_np\n",
    "\n",
    "def convert_rgb_to_ycbcr(image_np):\n",
    "    image_np = image_np.astype(np.uint8)\n",
    "    ycbcr_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2YCrCb)\n",
    "    return ycbcr_image\n",
    "'''\n",
    "def transform_using_watershed(grad_mag_np, image_np, labels_np):\n",
    "    grad_mag_np = cv2.convertScaleAbs(grad_mag_np)\n",
    "    ret, thresh = cv2.threshold(grad_mag_np, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "    \n",
    "    \n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "    markers = markers.astype('int32')\n",
    "    image_np = cv2.convertScaleAbs(image_np)\n",
    "    markers = cv2.watershed(image_np, markers)\n",
    "    img_output = np.zeros_like(image_np)\n",
    "    img_output[markers == -1] = 255\n",
    "    plt.imshow(img_output)\n",
    "    plt.show()\n",
    "'''\n",
    "\n",
    "def tensor_dataset_from_np(images_np, labels_np):\n",
    "    return tf.data.Dataset.from_tensor_slices((images_np,labels_np)).shuffle(buffer_size=1000).batch(batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image segmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbourhood_region_labelling(np_binary_image):\n",
    "    #labels is a numpy array that contains the labelled image. Each pixel is assigned a label value corresponding to the inex of the connected component that pixel belongs to\n",
    "    #num_labels is an integer representing number of connected components\n",
    "    num_labels, labels = cv2.connectedComponents(np_binary_image, connectivity=8)\n",
    "    return labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection and smoothing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_blur(images_np):\n",
    "    # second argument is kernel size. Larger the kernel size means wider distribution of weights\n",
    "    # third argument is sigma - larger the sigma the more spread out the blur effect. Pixels far away have greater influence on final output\n",
    "    # A larger kernel size can help to smooth out larger features in the image, while a larger sigma value can help to remove finer details and noise\n",
    "    blurred_image_np = cv2.GaussianBlur(images_np, (7, 7), 15)\n",
    "    return blurred_image_np\n",
    "\n",
    "def calculate_gradient_mag_using_sobel(grayscale_np):\n",
    "    #Second argument is the data type of the output gradient image\n",
    "    #Third - Order of derivative in x direction\n",
    "    #Fourth - Order of derivative in y direction\n",
    "    #Fifth - Sobel Kernel to be used in convolution\n",
    "    grad_x = cv2.Sobel(grayscale_np, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(grayscale_np, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    return grad_mag\n",
    "\n",
    "def calculate_gradient_mag_using_canny(grayscale_np, lower_thresh = 0, upper_thresh = 0):\n",
    "    #second argument - grad mag threshold. Any edge with grad mag lower than this is discarded as weak edges\n",
    "    #third argument - grad mag threshold but any edge with grad mag higher considered as strong edges\n",
    "    grayscale_np = grayscale_np.astype(np.uint8)\n",
    "    edges_np = cv2.Canny(grayscale_np,lower_thresh, upper_thresh)\n",
    "    return edges_np\n",
    "\n",
    "\n",
    "def get_contours_from_binary_edges(edges):\n",
    "    contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    return contours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision modelling and detecting shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_circles_from_edges(edges, min_radius = 0, max_radius = 100):\n",
    "    #first argument is the binary image of edges\n",
    "    #Second argument is variation of hough transform -\n",
    "    #variants include cv2.HOUGH_STANDARD, cv2.HOUGH_PROBABALISTIC, cv2.HOUGH_GRADIENT\n",
    "    #third argument is resolution of accumulator array - represents ratio between image resolution and resolution of accumulator array - determines granularity of search - smaller leads to more finer search\n",
    "    #fourth argument is the minimum distance between centers of detected circles\n",
    "    #param1 is the higher threshold for the canny edge detector\n",
    "    #fourth argument is Min and max radius of circles found\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1,20, param1=50, param2= 30, minRadius=min_radius, maxRadius=max_radius)\n",
    "    \n",
    "    if circles is not None:\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    "        distances = np.sqrt((circles[:, 0] - edges.shape[1]/2)**2 + (circles[:, 1] - edges.shape[0]/2)**2)\n",
    "        sorted_circles = circles[np.argsort(distances)]\n",
    "        sorted_circles = np.expand_dims(sorted_circles, axis=0)\n",
    "        return sorted_circles\n",
    "    \n",
    "    return circles\n",
    "\n",
    "\n",
    "def draw_circles_on_image(image_np, circles):\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        #because of break after this, it will only take the top most circle i.e. closest to centre\n",
    "        for i in circles[0, :]:\n",
    "            center = (i[0], i[1])\n",
    "            # circle center\n",
    "            cv2.circle(image_np, center, 1, (0, 100, 100), 3)\n",
    "            # circle outline\n",
    "            radius = i[2]\n",
    "            cv2.circle(image_np, center, radius, (255, 0, 255), 3)\n",
    "            break\n",
    "    return image_np\n",
    "\n",
    "def create_and_apply_circle_mask(image_np, circles):\n",
    "    if circles is not None:\n",
    "        circle = circles[0][0]\n",
    "        mask = np.zeros(image_np.shape[:2], dtype=np.uint8)\n",
    "        cv2.circle(mask, (circle[0], circle[1]), circle[2], (255, 255, 255), -1)\n",
    "        result = cv2.bitwise_and(image_np, image_np, mask=mask)\n",
    "    else:\n",
    "        result = np.zeros_like(image_np)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(images_np, labels_np):\n",
    "    images_np, labels_np = apply_gaussian_blur(images_np, labels_np)\n",
    "    data_from_np = tensor_dataset_from_np(images_np, labels_np)\n",
    "    \n",
    "for i in range(len(images_np)):\n",
    "    print_image(images_np[i])\n",
    "    #binary_np= apply_threshold_rgb(images_np[i], np.array([0,0,0]), np.array([250,250,250]))\n",
    "    #image_np = combine_mask_with_image(binary_np, images_np[i])\n",
    "    \n",
    "\n",
    "    ycbcr_image = convert_rgb_to_ycbcr(images_np[i])\n",
    "    binary_np = apply_threshold_rgb(ycbcr_image, np.array([22,130,110]), np.array([122,155,130]))\n",
    "    image_np = combine_mask_with_image(binary_np, images_np[i])\n",
    "    #print_image(image_np)\n",
    "    \n",
    "    \n",
    "    blurred_np= apply_gaussian_blur(images_np[i])\n",
    "    #print_image(blurred_np)\n",
    "    gray_np = convert_to_grayscale(blurred_np)\n",
    "    #print_image_grayscale(gray_np)\n",
    "    #grad_mag_np = calculate_gradient_mag_using_sobel(gray_np)\n",
    "    #print_image_grayscale(grad_mag_np, True)\n",
    "    grad_mag_np = calculate_gradient_mag_using_canny(gray_np, 20, 25)\n",
    "    #grad_mag_np = canny_skimage(gray_np)\n",
    "    print_image_grayscale(grad_mag_np, True)\n",
    "    circles = get_circles_from_edges(grad_mag_np, 50, 100)\n",
    "    to_draw_np = images_np[i]\n",
    "    masked_np = create_and_apply_circle_mask(images_np[i],circles)\n",
    "    print_image(masked_np)\n",
    "    #circles_np = draw_circle_on_image(to_draw_np, circles)\n",
    "    #circles_np = draw_circles_on_image(to_draw_np, circles)\n",
    "    #print_image(circles_np)\n",
    "    #perform_hough_transform(grad_mag_np, images_np[i])\n",
    "    #transform_using_watershed(grad_mag_np, images_np[i], labels_np[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_size = int(len(data) * .7)\n",
    "val_size = int(len(data) * .2)\n",
    "# +1 to make sure total is 12 as that was total no. of batches\n",
    "test_size = int(len(data) * .1) + 1\n",
    "\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_np_array_from_dataset(ds):\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "    for images, labels in ds.unbatch():\n",
    "        images_list.append(images.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "    images_np = np.array(images_list)\n",
    "    labels_np = np.array(labels_list)\n",
    "    return images_np, labels_np\n",
    "\n",
    "\n",
    "train_images_np, train_labels_np = return_np_array_from_dataset(train)\n",
    "print(len(train_images_np), \", \", len(train_labels_np))\n",
    "val_images_np, val_labels_np = return_np_array_from_dataset(val)\n",
    "print(len(val_images_np), \", \", len(val_labels_np))\n",
    "test_images_np, test_labels_np = return_np_array_from_dataset(test)\n",
    "print(len(test_images_np), \", \", len(test_labels_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(\n",
    "    train_images_np,\n",
    "    train_labels_np,\n",
    "    shuffle=False,\n",
    "    batch_size=32\n",
    ")\n",
    "val_generator = val_datagen.flow(\n",
    "    val_images_np,\n",
    "    val_labels_np,\n",
    "    shuffle=False,\n",
    "    batch_size=32\n",
    ")\n",
    "test_generator = test_datagen.flow(\n",
    "    test_images_np,\n",
    "    test_labels_np,\n",
    "    shuffle=False,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View images and labels generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the generator in the next to choose which generator\n",
    "images_to_show, labels_to_show = next(test_generator)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=32//2)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    print(i, \" \", ax)\n",
    "    ax.imshow(images_to_show[i])\n",
    "    ax.set_title(f\"{labels_to_show[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = ResNet50(include_top=False, weights='imagenet',\n",
    "                 input_shape=(256, 256, 3), classes=2, pooling='avg')\n",
    "resnet_model = Sequential()\n",
    "resnet_model.add(model)\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "resnet_model.add(Dense(512, activation='relu'))\n",
    "resnet_model.add(Dense(1, activation='sigmoid'))\n",
    "resnet_model.compile('adam', loss=tf.losses.BinaryCrossentropy(),\n",
    "                     metrics=['accuracy'])\n",
    "resnet_model.summary()\n",
    "'''\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(resnet_model)\n",
    "# Conv2D(No. of filters, dimensions of filter, activation function, expected image size(only first time))\n",
    "model.add(Conv2D(32, (3, 3), 1, activation='relu',\n",
    "          padding=\"same\", input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), 1, activation='relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), 1, activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph for error and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "plt.plot(hist.history['accuracy'], color='green', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'],\n",
    "         color='black', label='val_accuracy')\n",
    "fig.suptitle('Loss & accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = BinaryAccuracy()\n",
    "\n",
    "for i in range(2):\n",
    "    x, y = test_generator.next()\n",
    "    y_pred = model.predict(x)\n",
    "    y_hat = []\n",
    "    for x in y_pred:\n",
    "        y_hat.append(x[0])\n",
    "    pre.update_state(y, y_hat)\n",
    "    re.update_state(y, y_hat)\n",
    "    acc.update_state(y, y_hat)\n",
    "\n",
    "print(\"Precision: \", pre.result().numpy())\n",
    "print(\"Recall: \", re.result().numpy())\n",
    "print(\"Accuracy: \", acc.result().numpy())\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('models', 'tonsil_detector.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae5ed6e59739fd9c8a6a8aa249c06f0d19f950643afb8c47bef30ff59ac336a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
